2025/08/28 14:11:32 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.20 (main, Oct  3 2024, 07:27:41) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1
    GPU 0: NVIDIA GeForce RTX 3090
    CUDA_HOME: None
    GCC: gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
    PyTorch: 2.1.1+cu118
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.1+cu118
    OpenCV: 4.10.0
    MMEngine: 0.10.5

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1
    deterministic: False
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/08/28 14:11:32 - mmengine - INFO - Config:
default_hooks = dict(
    checkpoint=dict(by_epoch=True, interval=1, type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(enable=False, type='VisualizationHook'))
default_scope = 'mmpretrain'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
file_name = 'xvlm_1xb24_geotext1652'
launcher = 'none'
load_from = None
log_level = 'INFO'
model = dict(
    bbox_head=dict(hidden_size=768, type='XVLM_BOXHead'),
    fast_match=True,
    init_cfg=dict(
        checkpoint='pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth',
        type='Pretrained'),
    itm_head=dict(
        cal_acc=True, hidden_size=768, type='XVLM_ITMHead', with_pooler=False),
    max_tokens=90,
    text_encoder=dict(
        med_config=dict(
            add_cross_attention=True,
            architectures=[
                'BertForMaskedLM',
            ],
            attention_probs_dropout_prob=0.1,
            encoder_width=1024,
            fusion_layer=6,
            hidden_act='gelu',
            hidden_dropout_prob=0.1,
            hidden_size=768,
            initializer_range=0.02,
            intermediate_size=3072,
            layer_norm_eps=1e-12,
            max_position_embeddings=512,
            model_type='bert',
            num_attention_heads=12,
            num_hidden_layers=12,
            pad_token_id=0,
            type_vocab_size=2,
            vocab_size=30522),
        type='XVLM_XBert'),
    text_proj=dict(in_features=768, out_features=256, type='Linear'),
    tokenizer_path='pretrain/bert-base-uncased',
    topk=64,
    train_max_words=90,
    type='XVLMRetrieval',
    val_max_words=90,
    vision_encoder=dict(
        ape=False,
        depths=[
            2,
            2,
            18,
            2,
        ],
        drop_path_rate=0.1,
        drop_rate=0.0,
        embed_dim=128,
        img_size=384,
        in_chans=3,
        mlp_ratio=4.0,
        num_heads=[
            4,
            8,
            16,
            32,
        ],
        patch_norm=True,
        patch_size=4,
        qkv_bias=True,
        type='XVLM_SwinTransformer',
        use_checkpoint=False,
        window_size=12),
    vision_proj=dict(in_features=1024, out_features=256, type='Linear'))
optim_wrapper = dict(
    optimizer=dict(lr=3e-05, type='AdamW', weight_decay=0.01),
    paramwise_cfg=dict(bias_decay_mult=0.0, norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = [
    dict(
        by_epoch=False,
        end=1000,
        end_factor=1,
        start_factor=0.001,
        type='LinearLR'),
    dict(
        begin=1000,
        by_epoch=False,
        end_factor=1e-10,
        start_factor=1,
        type='LinearLR'),
]
rand_increasing_policies = [
    dict(type='AutoContrast'),
    dict(type='Equalize'),
    dict(
        magnitude_key='magnitude',
        magnitude_range=(
            0.1,
            1.8,
        ),
        type='Brightness'),
    dict(
        magnitude_key='magnitude',
        magnitude_range=(
            0.1,
            1.8,
        ),
        type='Sharpness'),
]
randomness = dict(deterministic=False, seed=1)
resume = False
test_cfg = dict(
    cache_dir='/home/rh/Files/MS/geotext_exp2/xvlm_1xb24_geotext1652',
    fast_datainfo=True,
    fp16=True,
    i2t=True,
    load_cpu=True,
    type='HDCRetrievalTestLoop')
test_dataloader = dict(
    batch_size=64,
    collate_fn=dict(type='default_collate'),
    dataset=dict(
        ann_file='test_951_version.json',
        data_prefix=dict(img_path='images'),
        data_root='../datasets/GeoText1652_Dataset',
        pipeline=[
            dict(imdecode_backend='pillow', type='LoadImageFromFile'),
            dict(
                backend='pillow',
                interpolation='bicubic',
                scale=(
                    384,
                    384,
                ),
                type='Resize'),
            dict(keys='text', type='CleanCaption'),
            dict(
                algorithm_keys=[
                    'text',
                    'gt_text_id',
                    'gt_image_id',
                ],
                meta_keys=[
                    'image_id',
                ],
                type='PackInputs'),
        ],
        test_mode=True,
        type='GeoText1652Dataset'),
    num_workers=16,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(subsample_type='sequential', type='SequentialSampler'))
test_evaluator = dict(
    topk=(
        1,
        5,
        10,
    ), type='RetrievalRecall')
test_pipeline = [
    dict(imdecode_backend='pillow', type='LoadImageFromFile'),
    dict(
        backend='pillow',
        interpolation='bicubic',
        scale=(
            384,
            384,
        ),
        type='Resize'),
    dict(keys='text', type='CleanCaption'),
    dict(
        algorithm_keys=[
            'text',
            'gt_text_id',
            'gt_image_id',
        ],
        meta_keys=[
            'image_id',
        ],
        type='PackInputs'),
]
train_cfg = dict(max_epochs=6, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_size=16,
    collate_fn=dict(type='default_collate'),
    dataset=dict(
        ann_file='train.json',
        data_prefix=dict(img_path='images'),
        data_root='../datasets/GeoText1652_Dataset',
        pipeline=[
            dict(imdecode_backend='pillow', type='LoadImageFromFile'),
            dict(
                backend='pillow',
                interpolation='bicubic',
                scale=(
                    384,
                    384,
                ),
                type='Resize'),
            dict(
                magnitude_level=7,
                num_policies=2,
                policies=[
                    dict(type='AutoContrast'),
                    dict(type='Equalize'),
                    dict(
                        magnitude_key='magnitude',
                        magnitude_range=(
                            0.1,
                            1.8,
                        ),
                        type='Brightness'),
                    dict(
                        magnitude_key='magnitude',
                        magnitude_range=(
                            0.1,
                            1.8,
                        ),
                        type='Sharpness'),
                ],
                type='RandAugment'),
            dict(keys='text', type='CleanCaption'),
            dict(
                algorithm_keys=[
                    'text',
                    'is_matched',
                    'sentences',
                    'bboxes',
                ],
                meta_keys=[
                    'image_id',
                    'building_id',
                ],
                type='PackInputs'),
        ],
        test_mode=False,
        type='GeoText1652Dataset'),
    drop_last=True,
    num_workers=16,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(imdecode_backend='pillow', type='LoadImageFromFile'),
    dict(
        backend='pillow',
        interpolation='bicubic',
        scale=(
            384,
            384,
        ),
        type='Resize'),
    dict(
        magnitude_level=7,
        num_policies=2,
        policies=[
            dict(type='AutoContrast'),
            dict(type='Equalize'),
            dict(
                magnitude_key='magnitude',
                magnitude_range=(
                    0.1,
                    1.8,
                ),
                type='Brightness'),
            dict(
                magnitude_key='magnitude',
                magnitude_range=(
                    0.1,
                    1.8,
                ),
                type='Sharpness'),
        ],
        type='RandAugment'),
    dict(keys='text', type='CleanCaption'),
    dict(
        algorithm_keys=[
            'text',
            'is_matched',
            'sentences',
            'bboxes',
        ],
        meta_keys=[
            'image_id',
            'building_id',
        ],
        type='PackInputs'),
]
val_cfg = dict(
    cache_dir='/home/rh/Files/MS/geotext_exp2/xvlm_1xb24_geotext1652',
    fast_datainfo=True,
    fp16=True,
    i2t=False,
    load_cpu=True,
    type='HDCRetrievalValLoop')
val_dataloader = dict(
    batch_size=64,
    collate_fn=dict(type='default_collate'),
    dataset=dict(
        ann_file='test_951_version.json',
        data_prefix=dict(img_path='images'),
        data_root='../datasets/GeoText1652_Dataset',
        pipeline=[
            dict(imdecode_backend='pillow', type='LoadImageFromFile'),
            dict(
                backend='pillow',
                interpolation='bicubic',
                scale=(
                    384,
                    384,
                ),
                type='Resize'),
            dict(keys='text', type='CleanCaption'),
            dict(
                algorithm_keys=[
                    'text',
                    'gt_text_id',
                    'gt_image_id',
                ],
                meta_keys=[
                    'image_id',
                ],
                type='PackInputs'),
        ],
        test_mode=True,
        type='GeoText1652Dataset'),
    num_workers=16,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(subsample_type='sequential', type='SequentialSampler'))
val_evaluator = dict(
    topk=(
        1,
        5,
        10,
    ), type='RetrievalRecall')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    type='UniversalVisualizer', vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/xvlm_1xb24_geotext1652'

2025/08/28 14:11:34 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/08/28 14:11:34 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) VisualizationHook                  
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) VisualizationHook                  
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.patch_embed.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.patch_embed.norm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.patch_embed.norm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.0.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.0.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.0.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.0.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.0.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.0.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.0.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.0.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.1.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.1.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.1.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.1.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.1.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.1.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.1.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.blocks.1.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.downsample.norm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.0.downsample.norm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.0.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.0.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.0.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.0.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.0.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.0.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.0.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.0.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.1.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.1.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.1.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.1.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.1.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.1.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.1.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.blocks.1.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.downsample.norm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.1.downsample.norm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.0.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.0.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.0.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.0.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.0.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.0.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.0.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.0.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.1.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.1.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.1.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.1.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.1.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.1.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.1.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.1.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.2.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.2.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.2.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.2.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.2.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.2.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.2.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.2.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.3.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.3.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.3.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.3.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.3.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.3.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.3.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.3.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.4.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.4.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.4.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.4.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.4.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.4.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.4.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.4.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.5.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.5.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.5.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.5.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.5.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.5.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.5.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.5.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.6.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.6.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.6.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.6.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.6.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.6.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.6.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.6.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.7.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.7.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.7.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.7.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.7.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.7.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.7.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.7.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.8.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.8.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.8.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.8.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.8.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.8.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.8.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.8.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.9.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.9.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.9.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.9.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.9.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.9.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.9.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.9.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.10.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.10.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.10.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.10.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.10.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.10.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.10.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.10.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.11.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.11.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.11.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.11.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.11.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.11.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.11.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.11.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.12.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.12.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.12.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.12.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.12.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.12.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.12.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.12.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.13.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.13.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.13.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.13.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.13.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.13.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.13.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.13.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.14.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.14.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.14.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.14.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.14.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.14.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.14.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.14.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.15.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.15.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.15.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.15.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.15.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.15.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.15.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.15.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.16.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.16.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.16.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.16.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.16.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.16.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.16.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.16.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.17.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.17.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.17.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.17.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.17.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.17.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.17.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.blocks.17.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.downsample.norm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.2.downsample.norm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.0.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.0.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.0.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.0.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.0.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.0.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.0.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.0.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.1.norm1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.1.norm1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.1.attn.qkv.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.1.attn.proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.1.norm2.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.1.norm2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.1.mlp.fc1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.layers.3.blocks.1.mlp.fc2.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.norm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_encoder.norm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.embeddings.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.embeddings.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.0.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.1.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.2.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.3.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.4.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.5.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.crossattention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.crossattention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.crossattention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.crossattention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.6.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.crossattention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.crossattention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.crossattention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.crossattention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.7.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.crossattention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.crossattention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.crossattention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.crossattention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.8.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.crossattention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.crossattention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.crossattention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.crossattention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.9.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.crossattention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.crossattention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.crossattention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.crossattention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.10.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.attention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.attention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.attention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.attention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.attention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.attention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.crossattention.self.query.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.crossattention.self.key.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.crossattention.self.value.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.crossattention.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.intermediate.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.output.dense.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.output.LayerNorm.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_encoder.encoder.layer.11.output.LayerNorm.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- vision_proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- text_proj.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- itm_head.fc.0.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- itm_head.fc.1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- itm_head.fc.1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- itm_head.fc.3.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- bbox_head.fc.0.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- bbox_head.fc.1.weight:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- bbox_head.fc.1.bias:weight_decay=0.0
2025/08/28 14:11:36 - mmengine - INFO - paramwise_options -- bbox_head.fc.3.bias:weight_decay=0.0
2025/08/28 14:11:56 - mmengine - INFO - load model from: pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth
2025/08/28 14:11:56 - mmengine - INFO - Loads checkpoint by local backend from path: pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth
2025/08/28 14:11:56 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: text_encoder.cls.predictions.bias, text_encoder.cls.predictions.transform.dense.weight, text_encoder.cls.predictions.transform.dense.bias, text_encoder.cls.predictions.transform.LayerNorm.weight, text_encoder.cls.predictions.transform.LayerNorm.bias, text_encoder.cls.predictions.decoder.weight, text_encoder.cls.predictions.decoder.bias

missing keys in source state_dict: vision_encoder.layers.0.blocks.0.attn.relative_position_index, vision_encoder.layers.0.blocks.1.attn_mask, vision_encoder.layers.0.blocks.1.attn.relative_position_index, vision_encoder.layers.1.blocks.0.attn.relative_position_index, vision_encoder.layers.1.blocks.1.attn_mask, vision_encoder.layers.1.blocks.1.attn.relative_position_index, vision_encoder.layers.2.blocks.0.attn.relative_position_index, vision_encoder.layers.2.blocks.1.attn_mask, vision_encoder.layers.2.blocks.1.attn.relative_position_index, vision_encoder.layers.2.blocks.2.attn.relative_position_index, vision_encoder.layers.2.blocks.3.attn_mask, vision_encoder.layers.2.blocks.3.attn.relative_position_index, vision_encoder.layers.2.blocks.4.attn.relative_position_index, vision_encoder.layers.2.blocks.5.attn_mask, vision_encoder.layers.2.blocks.5.attn.relative_position_index, vision_encoder.layers.2.blocks.6.attn.relative_position_index, vision_encoder.layers.2.blocks.7.attn_mask, vision_encoder.layers.2.blocks.7.attn.relative_position_index, vision_encoder.layers.2.blocks.8.attn.relative_position_index, vision_encoder.layers.2.blocks.9.attn_mask, vision_encoder.layers.2.blocks.9.attn.relative_position_index, vision_encoder.layers.2.blocks.10.attn.relative_position_index, vision_encoder.layers.2.blocks.11.attn_mask, vision_encoder.layers.2.blocks.11.attn.relative_position_index, vision_encoder.layers.2.blocks.12.attn.relative_position_index, vision_encoder.layers.2.blocks.13.attn_mask, vision_encoder.layers.2.blocks.13.attn.relative_position_index, vision_encoder.layers.2.blocks.14.attn.relative_position_index, vision_encoder.layers.2.blocks.15.attn_mask, vision_encoder.layers.2.blocks.15.attn.relative_position_index, vision_encoder.layers.2.blocks.16.attn.relative_position_index, vision_encoder.layers.2.blocks.17.attn_mask, vision_encoder.layers.2.blocks.17.attn.relative_position_index, vision_encoder.layers.3.blocks.0.attn.relative_position_index, vision_encoder.layers.3.blocks.1.attn.relative_position_index

Name of parameter - Initialization information

temp - torch.Size([]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.patch_embed.proj.weight - torch.Size([128, 3, 4, 4]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.patch_embed.proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.patch_embed.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.patch_embed.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.attn.relative_position_bias_table - torch.Size([529, 4]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.attn.qkv.weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.attn.qkv.bias - torch.Size([384]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.attn.proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.attn.proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.mlp.fc1.weight - torch.Size([512, 128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.mlp.fc1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.mlp.fc2.weight - torch.Size([128, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.0.mlp.fc2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.attn.relative_position_bias_table - torch.Size([529, 4]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.attn.qkv.weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.attn.qkv.bias - torch.Size([384]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.attn.proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.attn.proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.mlp.fc1.weight - torch.Size([512, 128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.mlp.fc1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.mlp.fc2.weight - torch.Size([128, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.blocks.1.mlp.fc2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.downsample.reduction.weight - torch.Size([256, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.downsample.norm.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.0.downsample.norm.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.norm1.weight - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.norm1.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.attn.relative_position_bias_table - torch.Size([529, 8]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.attn.qkv.weight - torch.Size([768, 256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.attn.qkv.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.attn.proj.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.norm2.weight - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.norm2.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.0.mlp.fc2.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.norm1.weight - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.norm1.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.attn.relative_position_bias_table - torch.Size([529, 8]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.attn.qkv.weight - torch.Size([768, 256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.attn.qkv.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.attn.proj.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.norm2.weight - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.norm2.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.blocks.1.mlp.fc2.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.downsample.reduction.weight - torch.Size([512, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.downsample.norm.weight - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.1.downsample.norm.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.0.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.1.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.2.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.3.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.4.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.5.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.6.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.7.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.8.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.9.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.10.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.11.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.12.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.13.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.14.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.15.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.16.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.attn.relative_position_bias_table - torch.Size([529, 16]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.attn.qkv.weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.attn.qkv.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.attn.proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.attn.proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.mlp.fc1.weight - torch.Size([2048, 512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.mlp.fc1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.mlp.fc2.weight - torch.Size([512, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.blocks.17.mlp.fc2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.downsample.reduction.weight - torch.Size([1024, 2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.downsample.norm.weight - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.2.downsample.norm.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.attn.relative_position_bias_table - torch.Size([529, 32]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.0.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.attn.relative_position_bias_table - torch.Size([529, 32]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.layers.3.blocks.1.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.norm.weight - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_encoder.norm.bias - torch.Size([1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.embeddings.word_embeddings.weight - torch.Size([30522, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.embeddings.position_embeddings.weight - torch.Size([512, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.embeddings.token_type_embeddings.weight - torch.Size([2, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.embeddings.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.embeddings.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.0.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.1.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.2.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.3.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.4.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.5.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.self.key.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.self.value.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.6.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.self.key.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.self.value.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.7.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.self.key.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.self.value.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.8.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.self.key.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.self.value.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.9.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.self.key.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.self.value.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.10.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.self.key.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.self.value.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.attention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.self.query.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.self.query.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.self.key.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.self.key.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.self.value.weight - torch.Size([768, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.self.value.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.output.dense.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.intermediate.dense.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.intermediate.dense.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.output.dense.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.output.dense.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.output.LayerNorm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_encoder.encoder.layer.11.output.LayerNorm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_proj.weight - torch.Size([256, 1024]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

vision_proj.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_proj.weight - torch.Size([256, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

text_proj.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

itm_head.fc.0.weight - torch.Size([1536, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

itm_head.fc.0.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

itm_head.fc.1.weight - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

itm_head.fc.1.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

itm_head.fc.3.weight - torch.Size([2, 1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

itm_head.fc.3.bias - torch.Size([2]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

bbox_head.fc.0.weight - torch.Size([1536, 768]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

bbox_head.fc.0.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

bbox_head.fc.1.weight - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

bbox_head.fc.1.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

bbox_head.fc.3.weight - torch.Size([4, 1536]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 

bbox_head.fc.3.bias - torch.Size([4]): 
PretrainedInit: load from pretrain/16m_base_model_state_step_199999_(xvlm2mmcv).pth 
2025/08/28 14:11:56 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/08/28 14:11:56 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/08/28 14:11:56 - mmengine - INFO - Checkpoints will be saved to /home/rh/Files/MS/HCCM/work_dirs/xvlm_1xb24_geotext1652.
2025/08/28 14:12:39 - mmengine - INFO - Epoch(train) [1][  50/9415]  base_lr: 1.5000e-06 lr: 1.5000e-06  eta: 13:26:10  time: 0.8339  data_time: 0.0081  memory: 18151  loss: 3.8405  bboxes_loss: 0.1612  itm_loss: 0.6523  itm_accuracy: 66.6667  itc_loss: 3.0270
2025/08/28 14:13:20 - mmengine - INFO - Epoch(train) [1][ 100/9415]  base_lr: 3.0000e-06 lr: 3.0000e-06  eta: 13:12:56  time: 0.8458  data_time: 0.0079  memory: 18202  loss: 3.6211  bboxes_loss: 0.1488  itm_loss: 0.6406  itm_accuracy: 66.6667  itc_loss: 2.8317
2025/08/28 14:14:02 - mmengine - INFO - Epoch(train) [1][ 150/9415]  base_lr: 4.5000e-06 lr: 4.5000e-06  eta: 13:07:50  time: 0.8453  data_time: 0.0070  memory: 18243  loss: 3.4793  bboxes_loss: 0.1458  itm_loss: 0.6362  itm_accuracy: 66.6667  itc_loss: 2.6974
2025/08/28 14:14:44 - mmengine - INFO - Epoch(train) [1][ 200/9415]  base_lr: 6.0000e-06 lr: 6.0000e-06  eta: 13:09:20  time: 0.8662  data_time: 0.0075  memory: 18162  loss: 3.4330  bboxes_loss: 0.1513  itm_loss: 0.6342  itm_accuracy: 66.6667  itc_loss: 2.6475
2025/08/28 14:15:26 - mmengine - INFO - Epoch(train) [1][ 250/9415]  base_lr: 7.5000e-06 lr: 7.5000e-06  eta: 13:06:39  time: 0.8611  data_time: 0.0077  memory: 18293  loss: 3.2811  bboxes_loss: 0.1530  itm_loss: 0.6272  itm_accuracy: 68.7500  itc_loss: 2.5008
2025/08/28 14:16:08 - mmengine - INFO - Epoch(train) [1][ 300/9415]  base_lr: 9.0000e-06 lr: 9.0000e-06  eta: 13:06:39  time: 0.8531  data_time: 0.0074  memory: 18245  loss: 2.8101  bboxes_loss: 0.1442  itm_loss: 0.6111  itm_accuracy: 66.6667  itc_loss: 2.0548
2025/08/28 14:16:50 - mmengine - INFO - Epoch(train) [1][ 350/9415]  base_lr: 1.0500e-05 lr: 1.0500e-05  eta: 13:06:01  time: 0.8432  data_time: 0.0069  memory: 18099  loss: 2.5959  bboxes_loss: 0.1461  itm_loss: 0.5863  itm_accuracy: 68.7500  itc_loss: 1.8635
2025/08/28 14:17:32 - mmengine - INFO - Epoch(train) [1][ 400/9415]  base_lr: 1.2000e-05 lr: 1.2000e-05  eta: 13:05:08  time: 0.8175  data_time: 0.0065  memory: 18279  loss: 2.4877  bboxes_loss: 0.1350  itm_loss: 0.5772  itm_accuracy: 72.9167  itc_loss: 1.7755
2025/08/28 14:18:14 - mmengine - INFO - Epoch(train) [1][ 450/9415]  base_lr: 1.3500e-05 lr: 1.3500e-05  eta: 13:04:18  time: 0.8511  data_time: 0.0069  memory: 18130  loss: 2.4328  bboxes_loss: 0.1156  itm_loss: 0.5797  itm_accuracy: 68.7500  itc_loss: 1.7375
2025/08/28 14:18:55 - mmengine - INFO - Epoch(train) [1][ 500/9415]  base_lr: 1.5000e-05 lr: 1.5000e-05  eta: 13:02:40  time: 0.8262  data_time: 0.0066  memory: 18071  loss: 2.4316  bboxes_loss: 0.1050  itm_loss: 0.5859  itm_accuracy: 72.9167  itc_loss: 1.7407
2025/08/28 14:19:37 - mmengine - INFO - Epoch(train) [1][ 550/9415]  base_lr: 1.6500e-05 lr: 1.6500e-05  eta: 13:01:49  time: 0.8514  data_time: 0.0071  memory: 18227  loss: 2.3078  bboxes_loss: 0.1110  itm_loss: 0.5947  itm_accuracy: 70.8333  itc_loss: 1.6021
2025/08/28 14:20:19 - mmengine - INFO - Epoch(train) [1][ 600/9415]  base_lr: 1.8000e-05 lr: 1.8000e-05  eta: 13:01:08  time: 0.8384  data_time: 0.0069  memory: 18251  loss: 2.2722  bboxes_loss: 0.1104  itm_loss: 0.5724  itm_accuracy: 68.7500  itc_loss: 1.5895
2025/08/28 14:21:01 - mmengine - INFO - Epoch(train) [1][ 650/9415]  base_lr: 1.9500e-05 lr: 1.9500e-05  eta: 13:00:30  time: 0.8239  data_time: 0.0064  memory: 18112  loss: 1.9939  bboxes_loss: 0.1114  itm_loss: 0.5653  itm_accuracy: 68.7500  itc_loss: 1.3173
2025/08/28 14:21:43 - mmengine - INFO - Epoch(train) [1][ 700/9415]  base_lr: 2.1000e-05 lr: 2.1000e-05  eta: 12:59:36  time: 0.8214  data_time: 0.0065  memory: 18197  loss: 2.0640  bboxes_loss: 0.1059  itm_loss: 0.5822  itm_accuracy: 66.6667  itc_loss: 1.3760
2025/08/28 14:22:26 - mmengine - INFO - Epoch(train) [1][ 750/9415]  base_lr: 2.2500e-05 lr: 2.2500e-05  eta: 12:59:57  time: 0.8694  data_time: 0.0076  memory: 18227  loss: 2.1624  bboxes_loss: 0.1127  itm_loss: 0.5904  itm_accuracy: 75.0000  itc_loss: 1.4592
2025/08/28 14:23:08 - mmengine - INFO - Epoch(train) [1][ 800/9415]  base_lr: 2.4000e-05 lr: 2.4000e-05  eta: 12:59:44  time: 0.8461  data_time: 0.0070  memory: 18148  loss: 1.8248  bboxes_loss: 0.1092  itm_loss: 0.5188  itm_accuracy: 77.0833  itc_loss: 1.1968
2025/08/28 14:23:50 - mmengine - INFO - Epoch(train) [1][ 850/9415]  base_lr: 2.5500e-05 lr: 2.5500e-05  eta: 12:59:06  time: 0.8340  data_time: 0.0066  memory: 18115  loss: 1.7865  bboxes_loss: 0.1093  itm_loss: 0.5129  itm_accuracy: 79.1667  itc_loss: 1.1643
2025/08/28 14:24:32 - mmengine - INFO - Epoch(train) [1][ 900/9415]  base_lr: 2.7000e-05 lr: 2.7000e-05  eta: 12:58:25  time: 0.8525  data_time: 0.0071  memory: 18183  loss: 1.8772  bboxes_loss: 0.1071  itm_loss: 0.5647  itm_accuracy: 75.0000  itc_loss: 1.2053
2025/08/28 14:25:15 - mmengine - INFO - Epoch(train) [1][ 950/9415]  base_lr: 2.8500e-05 lr: 2.8500e-05  eta: 12:58:14  time: 0.8766  data_time: 0.0080  memory: 18267  loss: 1.9993  bboxes_loss: 0.1060  itm_loss: 0.5582  itm_accuracy: 79.1667  itc_loss: 1.3351
2025/08/28 14:25:57 - mmengine - INFO - Exp name: xvlm_1xb24_geotext1652_20250828_141131
2025/08/28 14:25:57 - mmengine - INFO - Epoch(train) [1][1000/9415]  base_lr: 3.0000e-05 lr: 3.0000e-05  eta: 12:57:19  time: 0.8256  data_time: 0.0066  memory: 18084  loss: 1.9220  bboxes_loss: 0.1131  itm_loss: 0.5415  itm_accuracy: 70.8333  itc_loss: 1.2673
2025/08/28 14:26:39 - mmengine - INFO - Epoch(train) [1][1050/9415]  base_lr: 2.9974e-05 lr: 2.9974e-05  eta: 12:56:51  time: 0.8154  data_time: 0.0064  memory: 18263  loss: 1.9525  bboxes_loss: 0.1164  itm_loss: 0.5256  itm_accuracy: 77.0833  itc_loss: 1.3105
2025/08/28 14:27:21 - mmengine - INFO - Epoch(train) [1][1100/9415]  base_lr: 2.9946e-05 lr: 2.9946e-05  eta: 12:56:12  time: 0.8420  data_time: 0.0067  memory: 18442  loss: 1.9347  bboxes_loss: 0.1118  itm_loss: 0.5532  itm_accuracy: 70.8333  itc_loss: 1.2698
2025/08/28 14:28:03 - mmengine - INFO - Epoch(train) [1][1150/9415]  base_lr: 2.9919e-05 lr: 2.9919e-05  eta: 12:55:28  time: 0.8229  data_time: 0.0062  memory: 18312  loss: 1.9367  bboxes_loss: 0.1009  itm_loss: 0.5360  itm_accuracy: 68.7500  itc_loss: 1.2998
2025/08/28 14:28:45 - mmengine - INFO - Epoch(train) [1][1200/9415]  base_lr: 2.9892e-05 lr: 2.9892e-05  eta: 12:55:01  time: 0.8177  data_time: 0.0065  memory: 18192  loss: 1.7670  bboxes_loss: 0.1166  itm_loss: 0.5203  itm_accuracy: 77.0833  itc_loss: 1.1300
2025/08/28 14:29:27 - mmengine - INFO - Epoch(train) [1][1250/9415]  base_lr: 2.9865e-05 lr: 2.9865e-05  eta: 12:53:49  time: 0.8460  data_time: 0.0071  memory: 18068  loss: 1.8577  bboxes_loss: 0.1112  itm_loss: 0.5339  itm_accuracy: 72.9167  itc_loss: 1.2126
2025/08/28 14:30:09 - mmengine - INFO - Epoch(train) [1][1300/9415]  base_lr: 2.9838e-05 lr: 2.9838e-05  eta: 12:53:07  time: 0.8442  data_time: 0.0068  memory: 18226  loss: 1.7791  bboxes_loss: 0.1052  itm_loss: 0.5207  itm_accuracy: 77.0833  itc_loss: 1.1532
2025/08/28 14:30:51 - mmengine - INFO - Epoch(train) [1][1350/9415]  base_lr: 2.9811e-05 lr: 2.9811e-05  eta: 12:52:16  time: 0.8287  data_time: 0.0066  memory: 18144  loss: 1.5188  bboxes_loss: 0.1023  itm_loss: 0.4737  itm_accuracy: 83.3333  itc_loss: 0.9428
2025/08/28 14:31:33 - mmengine - INFO - Epoch(train) [1][1400/9415]  base_lr: 2.9784e-05 lr: 2.9784e-05  eta: 12:51:34  time: 0.8325  data_time: 0.0068  memory: 18161  loss: 1.7779  bboxes_loss: 0.1086  itm_loss: 0.5362  itm_accuracy: 68.7500  itc_loss: 1.1331
2025/08/28 14:32:14 - mmengine - INFO - Epoch(train) [1][1450/9415]  base_lr: 2.9757e-05 lr: 2.9757e-05  eta: 12:50:40  time: 0.8294  data_time: 0.0068  memory: 18119  loss: 1.6954  bboxes_loss: 0.1052  itm_loss: 0.5326  itm_accuracy: 64.5833  itc_loss: 1.0576
2025/08/28 14:32:56 - mmengine - INFO - Epoch(train) [1][1500/9415]  base_lr: 2.9730e-05 lr: 2.9730e-05  eta: 12:50:06  time: 0.8249  data_time: 0.0067  memory: 18130  loss: 1.6415  bboxes_loss: 0.1109  itm_loss: 0.4904  itm_accuracy: 87.5000  itc_loss: 1.0402
2025/08/28 14:33:38 - mmengine - INFO - Epoch(train) [1][1550/9415]  base_lr: 2.9703e-05 lr: 2.9703e-05  eta: 12:49:12  time: 0.8303  data_time: 0.0066  memory: 18204  loss: 1.6579  bboxes_loss: 0.1000  itm_loss: 0.5257  itm_accuracy: 72.9167  itc_loss: 1.0322
2025/08/28 14:34:20 - mmengine - INFO - Epoch(train) [1][1600/9415]  base_lr: 2.9676e-05 lr: 2.9676e-05  eta: 12:48:34  time: 0.8313  data_time: 0.0065  memory: 18076  loss: 1.7078  bboxes_loss: 0.1034  itm_loss: 0.5099  itm_accuracy: 79.1667  itc_loss: 1.0945
2025/08/28 14:35:02 - mmengine - INFO - Epoch(train) [1][1650/9415]  base_lr: 2.9649e-05 lr: 2.9649e-05  eta: 12:47:31  time: 0.8149  data_time: 0.0063  memory: 18177  loss: 1.5686  bboxes_loss: 0.0981  itm_loss: 0.5148  itm_accuracy: 70.8333  itc_loss: 0.9557
2025/08/28 14:35:44 - mmengine - INFO - Epoch(train) [1][1700/9415]  base_lr: 2.9622e-05 lr: 2.9622e-05  eta: 12:46:53  time: 0.8554  data_time: 0.0073  memory: 18219  loss: 1.5890  bboxes_loss: 0.0993  itm_loss: 0.5028  itm_accuracy: 72.9167  itc_loss: 0.9869
2025/08/28 14:36:25 - mmengine - INFO - Epoch(train) [1][1750/9415]  base_lr: 2.9595e-05 lr: 2.9595e-05  eta: 12:45:59  time: 0.8233  data_time: 0.0064  memory: 18202  loss: 1.6765  bboxes_loss: 0.1107  itm_loss: 0.5228  itm_accuracy: 79.1667  itc_loss: 1.0431
2025/08/28 14:37:07 - mmengine - INFO - Epoch(train) [1][1800/9415]  base_lr: 2.9568e-05 lr: 2.9568e-05  eta: 12:45:10  time: 0.8354  data_time: 0.0070  memory: 18164  loss: 1.6861  bboxes_loss: 0.0983  itm_loss: 0.5120  itm_accuracy: 70.8333  itc_loss: 1.0758
2025/08/28 14:37:49 - mmengine - INFO - Epoch(train) [1][1850/9415]  base_lr: 2.9541e-05 lr: 2.9541e-05  eta: 12:44:22  time: 0.8277  data_time: 0.0066  memory: 18095  loss: 1.7599  bboxes_loss: 0.0978  itm_loss: 0.5531  itm_accuracy: 70.8333  itc_loss: 1.1090
2025/08/28 14:38:31 - mmengine - INFO - Epoch(train) [1][1900/9415]  base_lr: 2.9514e-05 lr: 2.9514e-05  eta: 12:43:49  time: 0.8621  data_time: 0.0075  memory: 18091  loss: 1.6682  bboxes_loss: 0.1037  itm_loss: 0.5156  itm_accuracy: 77.0833  itc_loss: 1.0488
2025/08/28 14:39:13 - mmengine - INFO - Epoch(train) [1][1950/9415]  base_lr: 2.9487e-05 lr: 2.9487e-05  eta: 12:43:05  time: 0.8537  data_time: 0.0071  memory: 18167  loss: 1.6922  bboxes_loss: 0.0952  itm_loss: 0.5343  itm_accuracy: 75.0000  itc_loss: 1.0627
2025/08/28 14:39:55 - mmengine - INFO - Exp name: xvlm_1xb24_geotext1652_20250828_141131
2025/08/28 14:39:55 - mmengine - INFO - Epoch(train) [1][2000/9415]  base_lr: 2.9460e-05 lr: 2.9460e-05  eta: 12:42:16  time: 0.8446  data_time: 0.0067  memory: 18291  loss: 1.7419  bboxes_loss: 0.1065  itm_loss: 0.5377  itm_accuracy: 77.0833  itc_loss: 1.0978
2025/08/28 14:40:37 - mmengine - INFO - Epoch(train) [1][2050/9415]  base_lr: 2.9433e-05 lr: 2.9433e-05  eta: 12:41:33  time: 0.8425  data_time: 0.0068  memory: 18179  loss: 1.6718  bboxes_loss: 0.1019  itm_loss: 0.5112  itm_accuracy: 79.1667  itc_loss: 1.0587
2025/08/28 14:41:19 - mmengine - INFO - Epoch(train) [1][2100/9415]  base_lr: 2.9406e-05 lr: 2.9406e-05  eta: 12:40:50  time: 0.8382  data_time: 0.0067  memory: 18258  loss: 1.7075  bboxes_loss: 0.1026  itm_loss: 0.5201  itm_accuracy: 77.0833  itc_loss: 1.0848
2025/08/28 14:42:00 - mmengine - INFO - Epoch(train) [1][2150/9415]  base_lr: 2.9379e-05 lr: 2.9379e-05  eta: 12:40:02  time: 0.8337  data_time: 0.0067  memory: 18242  loss: 1.5001  bboxes_loss: 0.1019  itm_loss: 0.4771  itm_accuracy: 81.2500  itc_loss: 0.9212
2025/08/28 14:42:42 - mmengine - INFO - Epoch(train) [1][2200/9415]  base_lr: 2.9352e-05 lr: 2.9352e-05  eta: 12:39:14  time: 0.8316  data_time: 0.0066  memory: 18118  loss: 1.7721  bboxes_loss: 0.1025  itm_loss: 0.5169  itm_accuracy: 70.8333  itc_loss: 1.1527
2025/08/28 14:43:25 - mmengine - INFO - Epoch(train) [1][2250/9415]  base_lr: 2.9325e-05 lr: 2.9325e-05  eta: 12:38:44  time: 0.8264  data_time: 0.0066  memory: 18157  loss: 1.6383  bboxes_loss: 0.1052  itm_loss: 0.5027  itm_accuracy: 75.0000  itc_loss: 1.0304
2025/08/28 14:44:06 - mmengine - INFO - Epoch(train) [1][2300/9415]  base_lr: 2.9298e-05 lr: 2.9298e-05  eta: 12:37:55  time: 0.8401  data_time: 0.0071  memory: 18130  loss: 1.6964  bboxes_loss: 0.1009  itm_loss: 0.5086  itm_accuracy: 72.9167  itc_loss: 1.0870
2025/08/28 14:44:48 - mmengine - INFO - Epoch(train) [1][2350/9415]  base_lr: 2.9271e-05 lr: 2.9271e-05  eta: 12:37:17  time: 0.8543  data_time: 0.0075  memory: 18272  loss: 1.4732  bboxes_loss: 0.1047  itm_loss: 0.4853  itm_accuracy: 68.7500  itc_loss: 0.8831
2025/08/28 14:45:31 - mmengine - INFO - Epoch(train) [1][2400/9415]  base_lr: 2.9244e-05 lr: 2.9244e-05  eta: 12:36:46  time: 0.8586  data_time: 0.0074  memory: 18340  loss: 1.5347  bboxes_loss: 0.1047  itm_loss: 0.4747  itm_accuracy: 79.1667  itc_loss: 0.9553
2025/08/28 14:46:13 - mmengine - INFO - Epoch(train) [1][2450/9415]  base_lr: 2.9217e-05 lr: 2.9217e-05  eta: 12:36:05  time: 0.8498  data_time: 0.0071  memory: 18151  loss: 1.6168  bboxes_loss: 0.1030  itm_loss: 0.5238  itm_accuracy: 72.9167  itc_loss: 0.9900
2025/08/28 14:46:55 - mmengine - INFO - Epoch(train) [1][2500/9415]  base_lr: 2.9190e-05 lr: 2.9190e-05  eta: 12:35:24  time: 0.8502  data_time: 0.0068  memory: 18215  loss: 1.8443  bboxes_loss: 0.1039  itm_loss: 0.5400  itm_accuracy: 68.7500  itc_loss: 1.2004
2025/08/28 14:47:37 - mmengine - INFO - Epoch(train) [1][2550/9415]  base_lr: 2.9163e-05 lr: 2.9163e-05  eta: 12:34:48  time: 0.8299  data_time: 0.0063  memory: 18182  loss: 1.5849  bboxes_loss: 0.1024  itm_loss: 0.5135  itm_accuracy: 83.3333  itc_loss: 0.9689
2025/08/28 14:48:19 - mmengine - INFO - Epoch(train) [1][2600/9415]  base_lr: 2.9136e-05 lr: 2.9136e-05  eta: 12:33:58  time: 0.8646  data_time: 0.0075  memory: 18252  loss: 1.6522  bboxes_loss: 0.0936  itm_loss: 0.5026  itm_accuracy: 72.9167  itc_loss: 1.0560
2025/08/28 14:49:00 - mmengine - INFO - Epoch(train) [1][2650/9415]  base_lr: 2.9108e-05 lr: 2.9108e-05  eta: 12:33:10  time: 0.8619  data_time: 0.0070  memory: 18244  loss: 1.6951  bboxes_loss: 0.1028  itm_loss: 0.5404  itm_accuracy: 77.0833  itc_loss: 1.0519
2025/08/28 14:49:42 - mmengine - INFO - Epoch(train) [1][2700/9415]  base_lr: 2.9081e-05 lr: 2.9081e-05  eta: 12:32:22  time: 0.8443  data_time: 0.0067  memory: 18109  loss: 1.5766  bboxes_loss: 0.1003  itm_loss: 0.4965  itm_accuracy: 72.9167  itc_loss: 0.9798
2025/08/28 14:50:25 - mmengine - INFO - Epoch(train) [1][2750/9415]  base_lr: 2.9054e-05 lr: 2.9054e-05  eta: 12:31:53  time: 0.8528  data_time: 0.0073  memory: 18241  loss: 1.3891  bboxes_loss: 0.1012  itm_loss: 0.4765  itm_accuracy: 70.8333  itc_loss: 0.8114
2025/08/28 14:51:07 - mmengine - INFO - Epoch(train) [1][2800/9415]  base_lr: 2.9027e-05 lr: 2.9027e-05  eta: 12:31:16  time: 0.8344  data_time: 0.0067  memory: 18206  loss: 1.5372  bboxes_loss: 0.0929  itm_loss: 0.5046  itm_accuracy: 72.9167  itc_loss: 0.9396
2025/08/28 14:51:49 - mmengine - INFO - Epoch(train) [1][2850/9415]  base_lr: 2.9000e-05 lr: 2.9000e-05  eta: 12:30:36  time: 0.8469  data_time: 0.0069  memory: 18213  loss: 1.4371  bboxes_loss: 0.1037  itm_loss: 0.4594  itm_accuracy: 79.1667  itc_loss: 0.8740
2025/08/28 14:52:31 - mmengine - INFO - Epoch(train) [1][2900/9415]  base_lr: 2.8973e-05 lr: 2.8973e-05  eta: 12:30:01  time: 0.8253  data_time: 0.0064  memory: 18323  loss: 1.4369  bboxes_loss: 0.0977  itm_loss: 0.4873  itm_accuracy: 79.1667  itc_loss: 0.8520
2025/08/28 14:53:13 - mmengine - INFO - Epoch(train) [1][2950/9415]  base_lr: 2.8946e-05 lr: 2.8946e-05  eta: 12:29:17  time: 0.8588  data_time: 0.0070  memory: 18160  loss: 1.5432  bboxes_loss: 0.0956  itm_loss: 0.4971  itm_accuracy: 75.0000  itc_loss: 0.9505
2025/08/28 14:53:55 - mmengine - INFO - Exp name: xvlm_1xb24_geotext1652_20250828_141131
2025/08/28 14:53:55 - mmengine - INFO - Epoch(train) [1][3000/9415]  base_lr: 2.8919e-05 lr: 2.8919e-05  eta: 12:28:36  time: 0.8322  data_time: 0.0067  memory: 18277  loss: 1.5788  bboxes_loss: 0.0979  itm_loss: 0.5129  itm_accuracy: 72.9167  itc_loss: 0.9679
2025/08/28 14:54:37 - mmengine - INFO - Epoch(train) [1][3050/9415]  base_lr: 2.8892e-05 lr: 2.8892e-05  eta: 12:27:51  time: 0.8032  data_time: 0.0058  memory: 18267  loss: 1.3974  bboxes_loss: 0.1000  itm_loss: 0.4678  itm_accuracy: 72.9167  itc_loss: 0.8295
2025/08/28 14:55:19 - mmengine - INFO - Epoch(train) [1][3100/9415]  base_lr: 2.8865e-05 lr: 2.8865e-05  eta: 12:27:08  time: 0.8473  data_time: 0.0070  memory: 18223  loss: 1.6056  bboxes_loss: 0.0921  itm_loss: 0.5011  itm_accuracy: 68.7500  itc_loss: 1.0124
2025/08/28 14:56:02 - mmengine - INFO - Epoch(train) [1][3150/9415]  base_lr: 2.8838e-05 lr: 2.8838e-05  eta: 12:26:37  time: 0.8524  data_time: 0.0073  memory: 18262  loss: 1.5458  bboxes_loss: 0.0967  itm_loss: 0.4991  itm_accuracy: 77.0833  itc_loss: 0.9500
2025/08/28 14:56:43 - mmengine - INFO - Epoch(train) [1][3200/9415]  base_lr: 2.8811e-05 lr: 2.8811e-05  eta: 12:25:47  time: 0.8381  data_time: 0.0066  memory: 18265  loss: 1.6145  bboxes_loss: 0.0984  itm_loss: 0.5024  itm_accuracy: 77.0833  itc_loss: 1.0136
2025/08/28 14:57:25 - mmengine - INFO - Epoch(train) [1][3250/9415]  base_lr: 2.8784e-05 lr: 2.8784e-05  eta: 12:25:06  time: 0.8560  data_time: 0.0078  memory: 18341  loss: 1.4132  bboxes_loss: 0.1052  itm_loss: 0.4575  itm_accuracy: 79.1667  itc_loss: 0.8505
2025/08/28 14:58:07 - mmengine - INFO - Epoch(train) [1][3300/9415]  base_lr: 2.8757e-05 lr: 2.8757e-05  eta: 12:24:24  time: 0.8125  data_time: 0.0064  memory: 18133  loss: 1.6141  bboxes_loss: 0.0983  itm_loss: 0.5147  itm_accuracy: 70.8333  itc_loss: 1.0012
2025/08/28 14:58:50 - mmengine - INFO - Epoch(train) [1][3350/9415]  base_lr: 2.8730e-05 lr: 2.8730e-05  eta: 12:23:50  time: 0.8323  data_time: 0.0069  memory: 18281  loss: 1.5634  bboxes_loss: 0.0905  itm_loss: 0.4722  itm_accuracy: 81.2500  itc_loss: 1.0008
2025/08/28 14:59:32 - mmengine - INFO - Epoch(train) [1][3400/9415]  base_lr: 2.8703e-05 lr: 2.8703e-05  eta: 12:23:14  time: 0.8651  data_time: 0.0077  memory: 18155  loss: 1.3063  bboxes_loss: 0.0876  itm_loss: 0.4780  itm_accuracy: 79.1667  itc_loss: 0.7407
2025/08/28 15:00:14 - mmengine - INFO - Epoch(train) [1][3450/9415]  base_lr: 2.8676e-05 lr: 2.8676e-05  eta: 12:22:39  time: 0.8511  data_time: 0.0072  memory: 18190  loss: 1.6382  bboxes_loss: 0.0975  itm_loss: 0.5218  itm_accuracy: 68.7500  itc_loss: 1.0189
2025/08/28 15:00:56 - mmengine - INFO - Epoch(train) [1][3500/9415]  base_lr: 2.8649e-05 lr: 2.8649e-05  eta: 12:21:54  time: 0.8466  data_time: 0.0069  memory: 18175  loss: 1.5350  bboxes_loss: 0.0925  itm_loss: 0.5111  itm_accuracy: 77.0833  itc_loss: 0.9315
2025/08/28 15:01:38 - mmengine - INFO - Epoch(train) [1][3550/9415]  base_lr: 2.8622e-05 lr: 2.8622e-05  eta: 12:21:10  time: 0.8145  data_time: 0.0064  memory: 18232  loss: 1.7769  bboxes_loss: 0.0928  itm_loss: 0.5443  itm_accuracy: 68.7500  itc_loss: 1.1398
2025/08/28 15:02:20 - mmengine - INFO - Epoch(train) [1][3600/9415]  base_lr: 2.8595e-05 lr: 2.8595e-05  eta: 12:20:29  time: 0.8575  data_time: 0.0075  memory: 18259  loss: 1.4746  bboxes_loss: 0.0957  itm_loss: 0.4792  itm_accuracy: 70.8333  itc_loss: 0.8997
2025/08/28 15:03:02 - mmengine - INFO - Epoch(train) [1][3650/9415]  base_lr: 2.8568e-05 lr: 2.8568e-05  eta: 12:19:42  time: 0.8278  data_time: 0.0069  memory: 18089  loss: 1.6009  bboxes_loss: 0.1021  itm_loss: 0.4821  itm_accuracy: 91.6667  itc_loss: 1.0166
2025/08/28 15:03:43 - mmengine - INFO - Epoch(train) [1][3700/9415]  base_lr: 2.8541e-05 lr: 2.8541e-05  eta: 12:18:53  time: 0.8413  data_time: 0.0071  memory: 18128  loss: 1.6780  bboxes_loss: 0.0964  itm_loss: 0.5034  itm_accuracy: 75.0000  itc_loss: 1.0782
2025/08/28 15:04:26 - mmengine - INFO - Epoch(train) [1][3750/9415]  base_lr: 2.8514e-05 lr: 2.8514e-05  eta: 12:18:14  time: 0.8326  data_time: 0.0069  memory: 18254  loss: 1.5548  bboxes_loss: 0.0945  itm_loss: 0.5070  itm_accuracy: 81.2500  itc_loss: 0.9532
2025/08/28 15:05:08 - mmengine - INFO - Epoch(train) [1][3800/9415]  base_lr: 2.8487e-05 lr: 2.8487e-05  eta: 12:17:40  time: 0.8805  data_time: 0.0081  memory: 18231  loss: 1.5241  bboxes_loss: 0.0950  itm_loss: 0.4967  itm_accuracy: 75.0000  itc_loss: 0.9324
2025/08/28 15:05:50 - mmengine - INFO - Epoch(train) [1][3850/9415]  base_lr: 2.8460e-05 lr: 2.8460e-05  eta: 12:16:56  time: 0.8382  data_time: 0.0068  memory: 18246  loss: 1.4581  bboxes_loss: 0.0927  itm_loss: 0.4999  itm_accuracy: 81.2500  itc_loss: 0.8655
2025/08/28 15:06:33 - mmengine - INFO - Epoch(train) [1][3900/9415]  base_lr: 2.8433e-05 lr: 2.8433e-05  eta: 12:16:22  time: 0.8519  data_time: 0.0069  memory: 18200  loss: 1.2978  bboxes_loss: 0.0886  itm_loss: 0.4529  itm_accuracy: 75.0000  itc_loss: 0.7563
2025/08/28 15:07:14 - mmengine - INFO - Epoch(train) [1][3950/9415]  base_lr: 2.8406e-05 lr: 2.8406e-05  eta: 12:15:36  time: 0.8420  data_time: 0.0068  memory: 18190  loss: 1.3865  bboxes_loss: 0.0915  itm_loss: 0.4702  itm_accuracy: 81.2500  itc_loss: 0.8248
2025/08/28 15:07:56 - mmengine - INFO - Exp name: xvlm_1xb24_geotext1652_20250828_141131
2025/08/28 15:07:56 - mmengine - INFO - Epoch(train) [1][4000/9415]  base_lr: 2.8379e-05 lr: 2.8379e-05  eta: 12:14:56  time: 0.8482  data_time: 0.0072  memory: 18151  loss: 1.5270  bboxes_loss: 0.1026  itm_loss: 0.4813  itm_accuracy: 75.0000  itc_loss: 0.9430
2025/08/28 15:08:38 - mmengine - INFO - Epoch(train) [1][4050/9415]  base_lr: 2.8352e-05 lr: 2.8352e-05  eta: 12:14:08  time: 0.8446  data_time: 0.0074  memory: 18184  loss: 1.4562  bboxes_loss: 0.1076  itm_loss: 0.4908  itm_accuracy: 81.2500  itc_loss: 0.8578
2025/08/28 15:09:20 - mmengine - INFO - Epoch(train) [1][4100/9415]  base_lr: 2.8325e-05 lr: 2.8325e-05  eta: 12:13:22  time: 0.8348  data_time: 0.0067  memory: 18176  loss: 1.5435  bboxes_loss: 0.0926  itm_loss: 0.5004  itm_accuracy: 77.0833  itc_loss: 0.9506
2025/08/28 15:10:02 - mmengine - INFO - Epoch(train) [1][4150/9415]  base_lr: 2.8298e-05 lr: 2.8298e-05  eta: 12:12:44  time: 0.8324  data_time: 0.0067  memory: 18098  loss: 1.5675  bboxes_loss: 0.0992  itm_loss: 0.5061  itm_accuracy: 79.1667  itc_loss: 0.9622
2025/08/28 15:10:44 - mmengine - INFO - Epoch(train) [1][4200/9415]  base_lr: 2.8270e-05 lr: 2.8270e-05  eta: 12:12:05  time: 0.8434  data_time: 0.0073  memory: 18135  loss: 1.4592  bboxes_loss: 0.1090  itm_loss: 0.4928  itm_accuracy: 87.5000  itc_loss: 0.8573
2025/08/28 15:11:26 - mmengine - INFO - Epoch(train) [1][4250/9415]  base_lr: 2.8243e-05 lr: 2.8243e-05  eta: 12:11:21  time: 0.8448  data_time: 0.0076  memory: 18140  loss: 1.3774  bboxes_loss: 0.0951  itm_loss: 0.4557  itm_accuracy: 81.2500  itc_loss: 0.8267
2025/08/28 15:12:09 - mmengine - INFO - Epoch(train) [1][4300/9415]  base_lr: 2.8216e-05 lr: 2.8216e-05  eta: 12:10:50  time: 0.8692  data_time: 0.0079  memory: 18296  loss: 1.4118  bboxes_loss: 0.0918  itm_loss: 0.4455  itm_accuracy: 79.1667  itc_loss: 0.8744
2025/08/28 15:12:51 - mmengine - INFO - Epoch(train) [1][4350/9415]  base_lr: 2.8189e-05 lr: 2.8189e-05  eta: 12:10:10  time: 0.8269  data_time: 0.0069  memory: 18174  loss: 1.6240  bboxes_loss: 0.1016  itm_loss: 0.5231  itm_accuracy: 83.3333  itc_loss: 0.9993
2025/08/28 15:13:33 - mmengine - INFO - Epoch(train) [1][4400/9415]  base_lr: 2.8162e-05 lr: 2.8162e-05  eta: 12:09:23  time: 0.8496  data_time: 0.0073  memory: 18133  loss: 1.6080  bboxes_loss: 0.0956  itm_loss: 0.5137  itm_accuracy: 66.6667  itc_loss: 0.9987
2025/08/28 15:14:15 - mmengine - INFO - Epoch(train) [1][4450/9415]  base_lr: 2.8135e-05 lr: 2.8135e-05  eta: 12:08:38  time: 0.8148  data_time: 0.0063  memory: 18210  loss: 1.5103  bboxes_loss: 0.0913  itm_loss: 0.5061  itm_accuracy: 83.3333  itc_loss: 0.9129
2025/08/28 15:14:56 - mmengine - INFO - Epoch(train) [1][4500/9415]  base_lr: 2.8108e-05 lr: 2.8108e-05  eta: 12:07:54  time: 0.8659  data_time: 0.0076  memory: 18145  loss: 1.4708  bboxes_loss: 0.0952  itm_loss: 0.4811  itm_accuracy: 75.0000  itc_loss: 0.8944
2025/08/28 15:15:38 - mmengine - INFO - Epoch(train) [1][4550/9415]  base_lr: 2.8081e-05 lr: 2.8081e-05  eta: 12:07:13  time: 0.8234  data_time: 0.0071  memory: 18100  loss: 1.3278  bboxes_loss: 0.1052  itm_loss: 0.4636  itm_accuracy: 66.6667  itc_loss: 0.7590
2025/08/28 15:16:21 - mmengine - INFO - Epoch(train) [1][4600/9415]  base_lr: 2.8054e-05 lr: 2.8054e-05  eta: 12:06:32  time: 0.8628  data_time: 0.0075  memory: 18175  loss: 1.4723  bboxes_loss: 0.0959  itm_loss: 0.4664  itm_accuracy: 81.2500  itc_loss: 0.9100
2025/08/28 15:17:03 - mmengine - INFO - Epoch(train) [1][4650/9415]  base_lr: 2.8027e-05 lr: 2.8027e-05  eta: 12:05:53  time: 0.8311  data_time: 0.0069  memory: 18124  loss: 1.3849  bboxes_loss: 0.1008  itm_loss: 0.4677  itm_accuracy: 79.1667  itc_loss: 0.8164
2025/08/28 15:17:45 - mmengine - INFO - Epoch(train) [1][4700/9415]  base_lr: 2.8000e-05 lr: 2.8000e-05  eta: 12:05:10  time: 0.8493  data_time: 0.0076  memory: 18155  loss: 1.3882  bboxes_loss: 0.0898  itm_loss: 0.4950  itm_accuracy: 83.3333  itc_loss: 0.8034
2025/08/28 15:18:27 - mmengine - INFO - Epoch(train) [1][4750/9415]  base_lr: 2.7973e-05 lr: 2.7973e-05  eta: 12:04:28  time: 0.8473  data_time: 0.0073  memory: 18153  loss: 1.6394  bboxes_loss: 0.0970  itm_loss: 0.5042  itm_accuracy: 85.4167  itc_loss: 1.0382
2025/08/28 15:19:09 - mmengine - INFO - Epoch(train) [1][4800/9415]  base_lr: 2.7946e-05 lr: 2.7946e-05  eta: 12:03:49  time: 0.8523  data_time: 0.0075  memory: 18202  loss: 1.3050  bboxes_loss: 0.0863  itm_loss: 0.4492  itm_accuracy: 85.4167  itc_loss: 0.7695
2025/08/28 15:19:51 - mmengine - INFO - Epoch(train) [1][4850/9415]  base_lr: 2.7919e-05 lr: 2.7919e-05  eta: 12:03:08  time: 0.8385  data_time: 0.0069  memory: 18337  loss: 1.4580  bboxes_loss: 0.0973  itm_loss: 0.4814  itm_accuracy: 79.1667  itc_loss: 0.8792
2025/08/28 15:20:33 - mmengine - INFO - Epoch(train) [1][4900/9415]  base_lr: 2.7892e-05 lr: 2.7892e-05  eta: 12:02:29  time: 0.8427  data_time: 0.0071  memory: 18221  loss: 1.4577  bboxes_loss: 0.1013  itm_loss: 0.4795  itm_accuracy: 72.9167  itc_loss: 0.8768
2025/08/28 15:21:15 - mmengine - INFO - Epoch(train) [1][4950/9415]  base_lr: 2.7865e-05 lr: 2.7865e-05  eta: 12:01:47  time: 0.8408  data_time: 0.0073  memory: 18191  loss: 1.3293  bboxes_loss: 0.0844  itm_loss: 0.4664  itm_accuracy: 83.3333  itc_loss: 0.7785
2025/08/28 15:21:57 - mmengine - INFO - Exp name: xvlm_1xb24_geotext1652_20250828_141131
2025/08/28 15:21:57 - mmengine - INFO - Epoch(train) [1][5000/9415]  base_lr: 2.7838e-05 lr: 2.7838e-05  eta: 12:01:02  time: 0.8272  data_time: 0.0068  memory: 18220  loss: 1.4256  bboxes_loss: 0.0997  itm_loss: 0.5059  itm_accuracy: 75.0000  itc_loss: 0.8201
2025/08/28 15:22:39 - mmengine - INFO - Epoch(train) [1][5050/9415]  base_lr: 2.7811e-05 lr: 2.7811e-05  eta: 12:00:22  time: 0.8426  data_time: 0.0071  memory: 18125  loss: 1.5174  bboxes_loss: 0.0897  itm_loss: 0.4763  itm_accuracy: 79.1667  itc_loss: 0.9514
2025/08/28 15:23:20 - mmengine - INFO - Epoch(train) [1][5100/9415]  base_lr: 2.7784e-05 lr: 2.7784e-05  eta: 11:59:31  time: 0.8162  data_time: 0.0065  memory: 18153  loss: 1.2078  bboxes_loss: 0.0988  itm_loss: 0.4285  itm_accuracy: 79.1667  itc_loss: 0.6805
2025/08/28 15:24:02 - mmengine - INFO - Epoch(train) [1][5150/9415]  base_lr: 2.7757e-05 lr: 2.7757e-05  eta: 11:58:43  time: 0.8246  data_time: 0.0068  memory: 18141  loss: 1.3531  bboxes_loss: 0.0937  itm_loss: 0.4689  itm_accuracy: 85.4167  itc_loss: 0.7905
2025/08/28 15:24:44 - mmengine - INFO - Epoch(train) [1][5200/9415]  base_lr: 2.7730e-05 lr: 2.7730e-05  eta: 11:58:04  time: 0.8748  data_time: 0.0081  memory: 18328  loss: 1.6050  bboxes_loss: 0.0895  itm_loss: 0.5263  itm_accuracy: 70.8333  itc_loss: 0.9891
2025/08/28 15:25:26 - mmengine - INFO - Epoch(train) [1][5250/9415]  base_lr: 2.7703e-05 lr: 2.7703e-05  eta: 11:57:22  time: 0.8250  data_time: 0.0068  memory: 18154  loss: 1.1568  bboxes_loss: 0.0892  itm_loss: 0.4399  itm_accuracy: 89.5833  itc_loss: 0.6277
2025/08/28 15:26:08 - mmengine - INFO - Epoch(train) [1][5300/9415]  base_lr: 2.7676e-05 lr: 2.7676e-05  eta: 11:56:41  time: 0.8264  data_time: 0.0066  memory: 18130  loss: 1.2082  bboxes_loss: 0.0947  itm_loss: 0.4346  itm_accuracy: 75.0000  itc_loss: 0.6789
2025/08/28 15:26:50 - mmengine - INFO - Epoch(train) [1][5350/9415]  base_lr: 2.7649e-05 lr: 2.7649e-05  eta: 11:56:00  time: 0.8470  data_time: 0.0070  memory: 18197  loss: 1.3046  bboxes_loss: 0.0906  itm_loss: 0.4970  itm_accuracy: 83.3333  itc_loss: 0.7170
2025/08/28 15:27:32 - mmengine - INFO - Epoch(train) [1][5400/9415]  base_lr: 2.7622e-05 lr: 2.7622e-05  eta: 11:55:16  time: 0.8234  data_time: 0.0068  memory: 18162  loss: 1.4194  bboxes_loss: 0.0844  itm_loss: 0.4673  itm_accuracy: 77.0833  itc_loss: 0.8677
2025/08/28 15:28:14 - mmengine - INFO - Epoch(train) [1][5450/9415]  base_lr: 2.7595e-05 lr: 2.7595e-05  eta: 11:54:34  time: 0.8398  data_time: 0.0071  memory: 18283  loss: 1.2327  bboxes_loss: 0.0967  itm_loss: 0.4403  itm_accuracy: 81.2500  itc_loss: 0.6957
2025/08/28 15:28:57 - mmengine - INFO - Epoch(train) [1][5500/9415]  base_lr: 2.7568e-05 lr: 2.7568e-05  eta: 11:54:00  time: 0.8803  data_time: 0.0083  memory: 18210  loss: 1.3497  bboxes_loss: 0.0970  itm_loss: 0.4479  itm_accuracy: 79.1667  itc_loss: 0.8048
2025/08/28 15:29:40 - mmengine - INFO - Epoch(train) [1][5550/9415]  base_lr: 2.7541e-05 lr: 2.7541e-05  eta: 11:53:22  time: 0.8614  data_time: 0.0080  memory: 18143  loss: 1.2587  bboxes_loss: 0.0922  itm_loss: 0.4430  itm_accuracy: 81.2500  itc_loss: 0.7235
2025/08/28 15:30:21 - mmengine - INFO - Epoch(train) [1][5600/9415]  base_lr: 2.7514e-05 lr: 2.7514e-05  eta: 11:52:35  time: 0.8084  data_time: 0.0065  memory: 18154  loss: 1.5161  bboxes_loss: 0.1078  itm_loss: 0.4889  itm_accuracy: 87.5000  itc_loss: 0.9193
2025/08/28 15:31:03 - mmengine - INFO - Epoch(train) [1][5650/9415]  base_lr: 2.7487e-05 lr: 2.7487e-05  eta: 11:51:53  time: 0.8241  data_time: 0.0067  memory: 18267  loss: 1.2960  bboxes_loss: 0.0899  itm_loss: 0.4650  itm_accuracy: 81.2500  itc_loss: 0.7410
2025/08/28 15:31:44 - mmengine - INFO - Epoch(train) [1][5700/9415]  base_lr: 2.7459e-05 lr: 2.7459e-05  eta: 11:51:07  time: 0.8244  data_time: 0.0069  memory: 18193  loss: 1.2798  bboxes_loss: 0.0984  itm_loss: 0.4162  itm_accuracy: 87.5000  itc_loss: 0.7653
2025/08/28 15:32:26 - mmengine - INFO - Epoch(train) [1][5750/9415]  base_lr: 2.7432e-05 lr: 2.7432e-05  eta: 11:50:22  time: 0.8765  data_time: 0.0078  memory: 18283  loss: 1.2550  bboxes_loss: 0.0939  itm_loss: 0.4555  itm_accuracy: 81.2500  itc_loss: 0.7055
2025/08/28 15:33:08 - mmengine - INFO - Epoch(train) [1][5800/9415]  base_lr: 2.7405e-05 lr: 2.7405e-05  eta: 11:49:37  time: 0.8226  data_time: 0.0070  memory: 18132  loss: 1.4815  bboxes_loss: 0.1065  itm_loss: 0.5065  itm_accuracy: 68.7500  itc_loss: 0.8685
2025/08/28 15:33:50 - mmengine - INFO - Epoch(train) [1][5850/9415]  base_lr: 2.7378e-05 lr: 2.7378e-05  eta: 11:48:54  time: 0.8292  data_time: 0.0067  memory: 18177  loss: 1.4953  bboxes_loss: 0.0973  itm_loss: 0.4744  itm_accuracy: 81.2500  itc_loss: 0.9237
2025/08/28 15:34:32 - mmengine - INFO - Epoch(train) [1][5900/9415]  base_lr: 2.7351e-05 lr: 2.7351e-05  eta: 11:48:16  time: 0.8133  data_time: 0.0062  memory: 18108  loss: 1.2602  bboxes_loss: 0.1070  itm_loss: 0.4612  itm_accuracy: 81.2500  itc_loss: 0.6920
2025/08/28 15:35:15 - mmengine - INFO - Epoch(train) [1][5950/9415]  base_lr: 2.7324e-05 lr: 2.7324e-05  eta: 11:47:38  time: 0.8429  data_time: 0.0068  memory: 18269  loss: 1.2792  bboxes_loss: 0.0922  itm_loss: 0.4537  itm_accuracy: 77.0833  itc_loss: 0.7333
2025/08/28 15:35:57 - mmengine - INFO - Exp name: xvlm_1xb24_geotext1652_20250828_141131
2025/08/28 15:35:57 - mmengine - INFO - Epoch(train) [1][6000/9415]  base_lr: 2.7297e-05 lr: 2.7297e-05  eta: 11:46:55  time: 0.8434  data_time: 0.0073  memory: 18212  loss: 1.4177  bboxes_loss: 0.1067  itm_loss: 0.4577  itm_accuracy: 77.0833  itc_loss: 0.8533
2025/08/28 15:36:39 - mmengine - INFO - Epoch(train) [1][6050/9415]  base_lr: 2.7270e-05 lr: 2.7270e-05  eta: 11:46:13  time: 0.8517  data_time: 0.0070  memory: 18184  loss: 1.3356  bboxes_loss: 0.0892  itm_loss: 0.4877  itm_accuracy: 75.0000  itc_loss: 0.7587
2025/08/28 15:37:21 - mmengine - INFO - Epoch(train) [1][6100/9415]  base_lr: 2.7243e-05 lr: 2.7243e-05  eta: 11:45:33  time: 0.8555  data_time: 0.0071  memory: 18188  loss: 1.4210  bboxes_loss: 0.0991  itm_loss: 0.4719  itm_accuracy: 68.7500  itc_loss: 0.8500
2025/08/28 15:38:03 - mmengine - INFO - Epoch(train) [1][6150/9415]  base_lr: 2.7216e-05 lr: 2.7216e-05  eta: 11:44:49  time: 0.8232  data_time: 0.0064  memory: 18243  loss: 1.3184  bboxes_loss: 0.0953  itm_loss: 0.4429  itm_accuracy: 87.5000  itc_loss: 0.7802
2025/08/28 15:38:45 - mmengine - INFO - Epoch(train) [1][6200/9415]  base_lr: 2.7189e-05 lr: 2.7189e-05  eta: 11:44:07  time: 0.7974  data_time: 0.0062  memory: 18193  loss: 1.3597  bboxes_loss: 0.0903  itm_loss: 0.4500  itm_accuracy: 75.0000  itc_loss: 0.8194
2025/08/28 15:39:26 - mmengine - INFO - Epoch(train) [1][6250/9415]  base_lr: 2.7162e-05 lr: 2.7162e-05  eta: 11:43:22  time: 0.8280  data_time: 0.0069  memory: 18099  loss: 1.5485  bboxes_loss: 0.0963  itm_loss: 0.5030  itm_accuracy: 75.0000  itc_loss: 0.9492
2025/08/28 15:40:09 - mmengine - INFO - Epoch(train) [1][6300/9415]  base_lr: 2.7135e-05 lr: 2.7135e-05  eta: 11:42:43  time: 0.8284  data_time: 0.0070  memory: 18190  loss: 1.4074  bboxes_loss: 0.1057  itm_loss: 0.4614  itm_accuracy: 81.2500  itc_loss: 0.8402
2025/08/28 15:40:51 - mmengine - INFO - Epoch(train) [1][6350/9415]  base_lr: 2.7108e-05 lr: 2.7108e-05  eta: 11:42:03  time: 0.8534  data_time: 0.0075  memory: 18145  loss: 1.3294  bboxes_loss: 0.0838  itm_loss: 0.4683  itm_accuracy: 85.4167  itc_loss: 0.7773
2025/08/28 15:41:32 - mmengine - INFO - Epoch(train) [1][6400/9415]  base_lr: 2.7081e-05 lr: 2.7081e-05  eta: 11:41:18  time: 0.8369  data_time: 0.0071  memory: 18253  loss: 1.6157  bboxes_loss: 0.0997  itm_loss: 0.5019  itm_accuracy: 79.1667  itc_loss: 1.0141
2025/08/28 15:42:14 - mmengine - INFO - Epoch(train) [1][6450/9415]  base_lr: 2.7054e-05 lr: 2.7054e-05  eta: 11:40:32  time: 0.8449  data_time: 0.0073  memory: 18212  loss: 1.1254  bboxes_loss: 0.0817  itm_loss: 0.4289  itm_accuracy: 77.0833  itc_loss: 0.6148
2025/08/28 15:42:56 - mmengine - INFO - Epoch(train) [1][6500/9415]  base_lr: 2.7027e-05 lr: 2.7027e-05  eta: 11:39:54  time: 0.8334  data_time: 0.0070  memory: 18209  loss: 1.4846  bboxes_loss: 0.0943  itm_loss: 0.4613  itm_accuracy: 77.0833  itc_loss: 0.9291
